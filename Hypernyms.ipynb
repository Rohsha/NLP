{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hypernyms.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOd8hUQIEo8LnK+y9kwrWA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohsha/NLP/blob/master/Hypernyms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ9nGzyEe-yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open(\"SimLex999-100.txt\",\"r\")\n",
        "lines=f.readlines()\n",
        "#lines=str(lines)\n",
        "result=[]\n",
        "from nltk import re\n",
        "lis1=[]\n",
        "for i in range(0,len(lines)) :\n",
        "    lis1=lis1+lines[i].split('\\t')\n",
        "lis1\n",
        "strtry=str(lis1)\n",
        "strtry=\" \".join(re.findall(\"[a-zA-Z]{2,}\", strtry))\n",
        "#print(strtry)\n",
        "las=strtry.split(' ')\n",
        "las\n",
        "sent1 = list()\n",
        "sent2 = list()\n",
        "index = 0\n",
        "for letter in las:\n",
        "    if index % 2 != 0:\n",
        "        sent2.append(letter)\n",
        "    else:\n",
        "        sent1.append(letter)\n",
        "    index += 1\n",
        "from nltk.corpus import wordnet\n",
        "syn_name1 = list()\n",
        "for i in range(0,len(sent1)):\n",
        "    syn = wordnet.synsets(sent1[i])[0] \n",
        "    syn_name1.append(syn.name())\n",
        "syn_name1\n",
        "from nltk.corpus import wordnet\n",
        "syn_name2 = list()\n",
        "for i in range(0,len(sent2)):\n",
        "    syn = wordnet.synsets(sent2[i])[0] \n",
        "    syn_name2.append(syn.name())\n",
        "sim=list()\n",
        "p1=list()\n",
        "p2=list()\n",
        "from nltk.corpus import wordnet as wn\n",
        "for i in range(0,101):\n",
        "    x=syn_name1[i]\n",
        "    y=syn_name2[i]\n",
        "    wd1=wn.synset(x)\n",
        "    wd2=wn.synset(y)\n",
        "    sim.append(wd1.path_similarity(wd2))\n",
        "sim\n",
        "ind=list()\n",
        "for i in sim:\n",
        "    if i is None:\n",
        "        continue\n",
        "    else:\n",
        "        ind.append(sim.index(i))\n",
        "p1=list()\n",
        "p2=list()\n",
        "fsim=list()\n",
        "for i in ind:\n",
        "    p1.append(sent1[i])\n",
        "    p2.append(sent2[i])\n",
        "    fsim.append(sim[i])\n",
        "import pandas as pd \n",
        "df = pd.DataFrame(list(zip(p1, p2,fsim)), \n",
        "               columns =['wd1', 'wd2','fSim']) \n",
        "df \n",
        "import numpy as np\n",
        "np.savetxt(r'BioSim-100-predicted.txt.txt',df ,fmt='%s')\n",
        "import nltk\n",
        "f1=open(\"text1.txt\",encoding=\"utf8\")\n",
        "lines=f1.readlines()\n",
        "lines=str(lines)\n",
        "import string\n",
        "lines.translate(str.maketrans('', '', string.punctuation))\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens=tokenizer.tokenize(lines)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "l1=list()\n",
        "for i in range(0,len(tagged)):\n",
        "    if tagged[i][1] == 'NN':\n",
        "        l1.append(tagged[i])\n",
        "l1=str(l1)\n",
        "l1=l1.replace(')',\"\")\n",
        "l1=l1.replace('(',\"\")\n",
        "l1=l1.replace('NN',\"\")\n",
        "w=list()\n",
        "w=l1.split(\" \")\n",
        "w\n",
        "p1=('answer','boy','answer','lady','room','thing','boy','state','pair','pride','heart','style','service','pair','perplexed','moment','furniture','time','bed','broom','breath','nothing','cat','beat','boy','door','stood','tomato','garden','voice','angle','distance','noise','time','boy','slack','roundabout','flight','thought','closet','look','mouth','truck','aunt','jam','let','switch','switch')\n",
        "p1=list(p1)\n",
        "p2=('air','lady','round','danger','lad','instant','board','fence','aunt','moment','laugh','anything','time','dog','saying','goodness','body','dander','minute','lick','duty','boy','truth','goodness','rod','child','sin','suffering','sister','boy','thing','heart','time','conscience','time','heart','well','man','born','woman','trouble','evening','afternoon','season','vanity','suspicion','evidence','inspiration')\n",
        "from nltk.corpus import wordnet\n",
        "syn_name1 = list()\n",
        "for i in range(0,len(p1)):\n",
        "    syn = wordnet.synsets(p1[i])[0] \n",
        "    syn_name1.append(syn.name())\n",
        "x1=list()\n",
        "n=int(len(syn_name1)/2)\n",
        "for i in range(0,n):\n",
        "    x1.append(syn_name1[i])\n",
        "    x2=list()\n",
        "n=int(len(syn_name1)/2)\n",
        "for i in range(n,len(syn_name1)):\n",
        "    x2.append(syn_name1[i])\n",
        "sim1=list()\n",
        "from nltk.corpus import wordnet as wn\n",
        "for i in range(0,n):\n",
        "    y1=x1[i]\n",
        "    y2=x2[i]\n",
        "    wd1=wn.synset(y1)\n",
        "    wd2=wn.synset(y2)\n",
        "    sim1.append(wd1.path_similarity(wd2))\n",
        "sim1 \n",
        "import pandas as pd \n",
        "df = pd.DataFrame(list(zip(p1, p2,sim1)), \n",
        "               columns =['wd1', 'wd2','sim']) \n",
        "df \n",
        "import numpy as np\n",
        "np.savetxt(r'original-pairs.txt',df ,fmt='%s')\n",
        "sim1=list()\n",
        "hyp1=list()\n",
        "hyp2=list()\n",
        "from nltk.corpus import wordnet as wn\n",
        "for i in range(0,n):\n",
        "    y1=x1[i]\n",
        "    y2=x2[i]\n",
        "    wd1=wn.synset(y1)\n",
        "    wd2=wn.synset(y2)\n",
        "    hyp1.append(wd1.hypernyms())\n",
        "    hyp2.append(wd2.hypernyms())\n",
        "fhyp1=list()\n",
        "fhyp2=list()\n",
        "for i in range(0,int(n/2)):\n",
        "    fhyp1.append(hyp1[i][0])\n",
        "fhyp2=list()\n",
        "for i in range(int(n/2),n):\n",
        "    fhyp2.append(hyp1[i][0])\n",
        "hl1=list()\n",
        "hl2=list()\n",
        "for i in range(0,int(n/2)):\n",
        "    w1=fhyp1[i]\n",
        "    w2=fhyp2[i]\n",
        "    hl1.append(w1.hypernyms())\n",
        "    hl2.append(w2.hypernyms())\n",
        "fl=list()\n",
        "fl1=list()\n",
        "hypsim=list()\n",
        "for i in range(0,12):\n",
        "    fl.append(hl1[i][0])\n",
        "    fl1.append(hl2[i][0])\n",
        "for i in range(0,12):\n",
        "    hp1=fl[i]\n",
        "    hp2=fl1[i]\n",
        "    hypsim.append(hp1.path_similarity(hp2))\n",
        "hypsim\n",
        "import pandas as pd \n",
        "df = pd.DataFrame(list(zip(fl, fl1,hypsim)), \n",
        "               columns =['wd1', 'wd2','sim']) \n",
        "df \n",
        "import numpy as np\n",
        "np.savetxt(r'original-pairs-hypernyms.txt',df ,fmt='%s')\n",
        "tl1=(\"thing\",\"pair\",\"pride\",\"perplexed\",\"bed\",\"nothing\",\"answer\",\"boy\",\"room\",\"beat\")\n",
        "tl2=(\"instant\",\"aunt\",\"moment\",\"saying\",\"minute\",\"boy\",\"air\",\"lady\",\"lad\",\"goodness\")\n",
        "tsim=[\"0.125\",\"0.1\",\"0.1\",\"0.1\",\"0.1\",\"0.1\",\"0.09090909090909091\",\"0.08333333333333333\",\"0.08333333333333333\",\"0.08333333333333333\"]\n",
        "import pandas as pd \n",
        "df = pd.DataFrame(list(zip(tl1, tl2,tsim)), \n",
        "               columns =['wd1', 'wd2','sim']) \n",
        "df \n",
        "import numpy as np\n",
        "np.savetxt(r' top.txt',df ,fmt='%s')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}