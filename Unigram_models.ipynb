{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unigram_models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpb3DI6t3KclucEiZb/c+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohsha/NLP/blob/master/Unigram_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfPChzPKfyrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "#Reading the files\n",
        "f = open(\"sampletest.txt\", \"r\")\n",
        "samtext=f.read()\n",
        "v = open(\"sampledata.vocab.txt\", \"r\")\n",
        "voc=v.read()\n",
        "#Tokenize the text\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "samt=samtext\n",
        "#Removing the <s> and </s>\n",
        "samtext=re.findall(\"<s>(.*)</s>\",samtext)\n",
        "samtext=str(samtext)\n",
        "tokens=tokenizer.tokenize(samtext)\n",
        "#tokens1=tokenizer.tokenize(samt)\n",
        "vocab=voc.replace('\\n',' ')\n",
        "vocabulary=tokenizer.tokenize(vocab)\n",
        "for i in range(0,len(tokens)):\n",
        "    if tokens[i]=='d' or tokens[i]=='e':\n",
        "        tokens[i]=\"UNK\"\n",
        "#Count the frequency of the tokens\n",
        "import collections\n",
        "from collections import Counter\n",
        "wordcounts = Counter(tokens)\n",
        "count_of_tokens=list(wordcounts.values())\n",
        "labels=list(wordcounts.keys())\n",
        "#compute P(wi) = count(wi )/ N\n",
        "prob=[x/19 for x in count_of_tokens]\n",
        "#Print as table\n",
        "import pandas\n",
        "pandas.DataFrame(prob,labels)\n",
        "#Laplace smoothing\n",
        "smooth=[x+1 for x in count_of_tokens]\n",
        "smooth\n",
        "smooth_prob=[x/19 for x in smooth]\n",
        "pandas.DataFrame(smooth_prob,,labels)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}